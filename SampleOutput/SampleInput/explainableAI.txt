Explainable AI – Not a black-box anymore
Abstract
With the increasing adoption of Artificial Intelligence solutions which use machine learning and deep-learning solution, there remain as concern about their black box nature. Understanding the working of these models and the reasons behind their predictions provide a sense of assurance and accountability around these models, establishing the critical factor of trust between the humans and machines especially in mission critical settings. Many frameworks have been proposed for incorporating explainability in the models which are being created for various solutions. While there are still many challenges in the scope and information content of these frameworks, they are an active area of research and development for the community as well as a priority for Mphasis as an organization given its and its clients nature of work. A few case-studies have also been detailed to provide a glimpse of the work in this area.
Challenges of a black-box AI
Artificial intelligence and machine learning as disciplines started around the same time as the software engineering. But with the AI winter in the intervening period of 1980s and 90’s, they didn’t get a chance to become part of mainstream projects. With the AI spring post 2010 facilitated by with emergence of cheap and powerful computing and abundant data, the machine learning and deep learning frameworks are gaining prominence in solving most of the problems. However as with all new technologies, they have their own set of teething problems which range from control, quality, scope to trust. Being a evolving field there is little by way of standardization both in terms of how the software codes get written and how problems get solved. On top of these, most (if not all) of the AI / ML models are human readable. When the scripts and algorithms are coded, these are to a large extent comprehensible to humans (definitely requiring a certain skill level), but when these algorithms start learning on the data provided to them, they become complete black-boxes comprising of just weights and biases which are quite difficult to decode for a human. This led to quite a few challenges which ultimately create hurdles in adoption of AI as well as our trust in them.
•	Technical Challenges – Since we don’t know what the algorithms are learning, a data scientist needs to rely on measuring the output of these models against the ground truth (tagged historical data). While this method is quite effective in tuning the models as per the existing dataset, they don’t give any indication of model behavior when presented with inputs which are anomalous with respect to the “normal” inputs. Just take this example. Deep Neural Networks (DNNs) which get used in self driving cars to identify objects and take decisions have been fooled [1] by simple adversarial attacks. A “Stop sign” when pasted over with few sticky tapes which humans have no problem recognizing gets identified as at 45 mph speed limit in a study.   Now we can just imagine its consequences at an intersection.
                                                                                        
                                                                                                         
•	Business Challenges – Businesses depend heavily on accountability and controllership for their sustainability. In fact, in a recent Gartner report [2] 13% of the respondents the inability of the enterprise to address governance issues around AI as it top challenge to adoption. Some of the industries like banking, insurance, healthcare which can benefit the most from AI based interventions are also heavily regulated and have found out that the black-box nature of the models make them non-compliant from many perspectives ranging from auditability to litigations arising out of accusation of biases. A University of California, Berkeley study [3] has found that certain racial groups were paying 5.6 – 8.6 basis points higher interests on purchase loans than other groups and 3 basis points more on refinance loans, when controlled for creditworthiness. To be fair the study found such discrimination by both human as well as algorithmic lenders. However, in case of algorithmic lending the accountability of such discriminations become fuzzier and legally challenging.
•	Ethical Challenges – Humans decision making is prone to cognitive biases.  Algorithms learn on data which were generated usually in manual processes and thus incorporate those biases. While human prejudiced decision making is itself of concern, they have the benefit of getting corrected when made aware of their biases. But the scale of decision making by a machine learning model makes them more prone to widespread prejudice which is covered by its volume of data generated and black-box nature of processes. This raises ethical questions in the use of such technologies. In certain US states algorithms are used to decide the risk scores of people for committing future crimes and given to judges deciding the case which influences the sentencing. However, such scores were found to be heavily biased towards against a certain racial group vs another [4]. This violates the basic principle of ethical conduct namely beneficence (“do no harm”).
All these challenges will keep on creating a significant barrier in the adoption of AI in all the walks of life. 
While we discussed the challenges, what makes the models so hard to understand. There are few reasons.
1.	Complexity – The models starting from general linear regression models or logistic regression models to Deep Neural networks have increasing level of complexity in their architectures. They can have complex feature interactions in their hidden layers. Or they can have non-monotonic relationships which make  understanding the patterns they represent difficult for humans to visualize.





                                   
2.	High Dimensionality – Human brain can visualize or understand things easily when they are in one or two dimensions. As the number of dimensions increase the power to interpret them falls rapidly. The number of features to a machine learning model decide its dimensionality. So as the number of features increase the models become less and less interpretable. Thus, even a simple algorithm like SVM will look like a black box if it learns over a high dimensional dataset.





			         
So, what does that mean for the machine learning models which is revolutionizing the world. Will they always have the handicap of being a black box which is not trustworthy? However, there is a silver lining in form of ongoing research and various frameworks available for making these opaque models more open to interpret their outputs.
The Explainable AI – available frameworks
Though it is a subject of ongoing research, but various techniques are already available for generating explanations for the predictions generated by the AI models. They generally are divided into two categories.
1.	The ante-hoc explainability - These are to a certain extent inherently transparent model. One example is the Bayesian Deep Learning which provides the confidence the model has in its given prediction. But this type of explainability limits the kind of algorithms which can be used and thus in turn limit the scope and accuracy of the solutions.
2.	The post-hoc explainability – These allow the explainability being incorporated into a wide range of models post training them. Thus, they are quite model agnostic and not limit us to using a few algorithms. We can pick algorithms, train our models and then incorporate explainability at the evaluation stage. They become an aid in identifying the best candidate models in terms of their prediction accuracy as well as best behaving at the same time. These frameworks are again divided into two major categories viz. local explanations and global explanations.
Local explanations or explanation by simplification are based on the rule extraction techniques. The most popular is the Local Interpretable Model-Agnostic Explanations (LIME) [5] and its variants like Anchor. LIME checks what happens to the predictions when we give variations of the test data into the machine learning model. LIME creates a new dataset consisting of perturbed samples and the corresponding predictions of the black box model. On this new dataset LIME then trains simpler interpretable model, which is weighted by the nearness of the perturbed instances to the instance of interest. Thus, it does not care about the behavior of the whole model but the behavior in the neighborhood of area of interest. 
  [5]
Global explanations or Feature relevance explanation strives to explain the working of a model by measuring the effect, weight or importance each feature has on the predicted output. One implementation is the SHAP [6] (SHapley Additive exPlanations). It uses a combination of an additive feature importance score for each prediction, and contribution each feature to prediction using coalition game theory and local gradients. It tries to ascertain how much has each feature value contributed to the prediction compared to the mean prediction? It works for both classification problems as well as regression. It has been implemented for a wide class or algorithms and has explainers for kernel-based algorithms, tree-based algorithms and deep-learning based algorithms. In fact, its deep explainer is a combined implementation of another DNN explainability framework DeepLIFT and the Shapley values. It provides multiple visualizations like force plots for individual case predictions, summary plots for model level feature importance, dependence plots for interaction between features etc.                                                           
                                                                    
               
					               
 
The Mphasis approach to Explainable AI
Mphasis has been an early starter in the pursuit of making the machine learning models more interpretable and open for scrutiny. The large base of its banking, financial services and insurance clients make it even more critical to prioritize model explainability in order to conform to various regulatory regimes. The research and innovation wing of Mphasis, NEXT Labs has been investing heavily in identifying suitable explainability frameworks which can work with the various models which are being developed for various use-cases in different domains. The organization strives to offer explainability features in all its critical models being developed for its clients along with the offer to build the components for interpretability of the existing models they (clients) are using. Some of the use-cases below highlight the capabilities of the organization in this journey.
Some Case-studies
Case Study 1 – A machine learning based solution was developed for Claims triaging for a multinational finance and insurance corporation. A multivariate feature-based (including text-based features) prediction model was trained on the data generated by its manual claims triaging process. While the model had a prediction accuracy of >98% with no false negatives, there was a need to understand the decision making process of the model as it was supporting a critical decision making process of either allowing settlement of the claim or further investigating it. The global explainability framework applied to it highlighted the importance of each of the feature in the prediction making. While scores of features were used for training the model, it turned out that the model was relying on only a few of them to generate the predictions. Also, the force plots for the case being processed was also generated in the real time providing a documented understanding of the factors which were being taken into consideration by the model for predicting the outcome of that case.


      
	 

 
Case Study 2 – The case pertains a solution created for a multinational media conglomerate for news aggregation and classification. The news around a particular entity (person or organization) was being aggregated from various sources and automatically classified in given categories using machine learning and natural language processing models. This classification was used by the associates to create research reports about these entities. A local interpretation framework was used to identify the key terms and phrases which were influencing the classification.
                 

Case Study 3 – An image analytics solution was created for a global courier company to identify damaged packages during transit. The solution used deep learning models to identify and classify parcel if they are damaged or not. Here too, a local explanation framework was used to identify the areas in the images which the model was looking at to classify them. The explanation consists of suspected areas being considered and the confidence the model has in classifying them as damaged / not damaged.
                 
                                          
                               

Next Steps
Model explainability is an active field of research where new developments are rapidly coming in. Mphasis continuously strives to keep itself abreast of these developments to solve for the needs of its clients. While the existing frameworks have helped make the opaque models more transparent still there are many challenges. The global explainability frameworks are not available for all the existing algorithms especially the boosted tree algos and many deep-learning frameworks. The local interpretability algorithms while being more flexible in this regard, are computationally intensive as they train new local models to create explanations. Also, the amount of information generated by the frameworks are somewhat limited.  Mphasis will continue working on two tracks
1.	Identifying the developments in frameworks which can help in increasing the coverage (more algorithms) and information content (more explanations).
2.	Provide Explainability as a Service by incorporating explainability in all the models being developed as well as on request for existing models. 	


References.	
1.	https://arxiv.org/abs/1707.08945
2.	https://www.gartner.com/smarterwithgartner/3-barriers-to-ai-adoption/
3.	http://faculty.haas.berkeley.edu/morse/research/papers/discrim.pdf
4.	https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing
5.	https://arxiv.org/abs/1602.04938
6.	https://github.com/slundberg/shap


